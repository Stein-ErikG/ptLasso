% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ptLassoMult.R
\name{ptLassoMult}
\alias{ptLassoMult}
\title{Fit a pretrained lasso model using glmnet.}
\usage{
ptLassoMult(
  x,
  y,
  alpha = 0.5,
  type.measure = c("default", "mse", "mae", "deviance"),
  overall.lambda = c("lambda.1se", "lambda.min"),
  overall.gamma = "gamma.1se",
  foldid = NULL,
  nfolds = 10,
  standardize = TRUE,
  verbose = FALSE,
  weights = NULL,
  penalty.factor = rep(1, nvars),
  fitoverall = NULL,
  fitind = NULL,
  en.alpha = 1,
  ...
)
}
\arguments{
\item{x}{input matrix, of dimension nobs x nvars; each row is an observation vector. Can be in sparse matrix format (inherit from class '"sparseMatrix"' as in package 'Matrix'). Requirement: 'nvars >1'; in other words, 'x' should have 2 or more columns.}

\item{y}{quantitative response variable, of dimension nobs x nresponses.}

\item{alpha}{The pretrained lasso hyperparameter, with \eqn{0\le\alpha\le 1}. The range of alpha is from 0 (which fits the overall model with fine tuning) to 1 (the individual models). The default value is 0.5, chosen mostly at random. To choose the appropriate value for your data, please either run \code{ptLasso} with a few choices of alpha and evaluate with a validation set, or use cv.ptLasso, which recommends a value of alpha using cross validation.}

\item{type.measure}{loss to use for cross-validation within each individual, overall, or pretrained lasso model. Choices are 'type.measure="mse"' (mean squared error), 'type.measure="mae"' (mean absolute error) and 'type.measure="deviance"'.}

\item{overall.lambda}{The choice of lambda to be used by the overall model to define the offset and penalty factor for pretrained lasso. Defaults to "lambda.1se", could alternatively be "lambda.min". This choice of lambda will be used to compute the offset and penalty factor (1) during model training and (2) during prediction. In the predict function, another lambda must be specified for the individual models, the second stage of pretraining and the overall model.}

\item{overall.gamma}{For use only when the option \code{relax = TRUE} is specified. The choice of gamma to be used by the overall model to define the offset and penalty factor for pretrained lasso. Defaults to "gamma.1se", but "gamma.min" is also a good option. This choice of gamma will be used to compute the offset and penalty factor (1) during model training and (2) during prediction. In the predict function, another gamma must be specified for the individual models, the second stage of pretraining and the overall model.}

\item{foldid}{An optional vector of values between 1 and \code{nfolds} identifying what fold each observation is in. If supplied, \code{nfold} can be missing.}

\item{nfolds}{Number of folds for CV (default is 10). Although \code{nfolds}can be as large as the sample size (leave-one-out CV), it is not recommended for large datasets. Smallest value allowable is \code{nfolds = 3}.}

\item{standardize}{Should the predictors be standardized before fitting (default is TRUE).}

\item{verbose}{If \code{verbose=1}, print a statement showing which model is currently being fit with \code{cv.glmnet}.}

\item{weights}{observation weights. Default is 1 for each observation.}

\item{penalty.factor}{Separate penalty factors can be applied to each coefficient. This is a number that multiplies 'lambda' to allow differential shrinkage. Can be 0 for some variables,  which implies no shrinkage, and that variable is always included in the model. Default is 1 for all variables (and implicitly infinity for variables listed in 'exclude'). For more information, see \code{?glmnet}. For pretraining, the user-supplied penalty.factor will be multiplied by the penalty.factor computed by the overall model.}

\item{fitoverall}{An optional cv.glmnet object specifying the overall model. This should have been trained on the full training data, with the argumnet keep = TRUE.}

\item{fitind}{An optional list of cv.glmnet objects specifying the individual models.}

\item{en.alpha}{The elasticnet mixing parameter, with 0 <= en.alpha <= 1. The penalty is defined as (1-alpha)/2||beta||_2^2+alpha||beta||_1. 'alpha=1' is the lasso penalty, and 'alpha=0' the ridge penalty. Default is `en.alpha = 1` (lasso).}

\item{\dots}{Additional arguments to be passed to the cv.glmnet functions. Notable choices include \code{"trace.it"} and \code{"parallel"}. If \code{trace.it = TRUE}, then a progress bar is displayed for each call to \code{"cv.glmnet"}; useful for big models that take a long time to fit. If \code{parallel = TRUE}, use parallel \code{foreach} to fit each fold.  Must register parallel before hand, such as \code{doMC} or others. ptLasso does not support the arguments \code{intercept}, \code{offset}, \code{fit} and \code{check.args}.}
}
\value{
An object of class \code{"ptLasso"}, which is a list with the ingredients of the fitted models.
\item{call}{The call that produced this object.}
\item{alpha}{The value of alpha used for pretraining.}
\item{fitoverall}{A fitted \code{cv.glmnet} object trained using the full data.}
\item{fitpre}{A list of fitted (pretrained) \code{cv.glmnet} objects, one trained for each response.}
\item{fitind}{A list of fitted \code{cv.glmnet} objects, one trained for each response.}
\item{fitoverall.lambda}{Lambda used with fitoverall, to compute the offset for pretraining.}
}
\description{
Fits a pretrained lasso model using the glmnet package, for a fixed choice of the pretraining hyperparameter alpha. Additionally fits an "overall" model (using all data) and "individual" models (use each individual group). Can fit input-grouped data with Gaussian, multinomial, binomial or Cox outcomes, and target-grouped data, which necessarily has a multinomial outcome. Many ptLasso arguments are passed directly to glmnet, and therefore the glmnet documentation is another good reference for ptLasso.
}
\examples{
# Getting started. First, we simulate data: we need covariates x and multiresponse y.
set.seed(1234)
n = 1000; ntrain = 500;
p = 500
sigma = 2
     
x = matrix(rnorm(n*p), n, p)
beta1 = c(rep(1, 5), rep(0.5, 5), rep(0, p - 10))
beta2 = c(rep(1, 5), rep(0, 5), rep(0.5, 5), rep(0, p - 15))

mu = cbind(x \%*\% beta1, x \%*\% beta2)
y  = cbind(mu[, 1] + sigma * rnorm(n), 
           mu[, 2] + sigma * rnorm(n))
cat("SNR for the two tasks:", round(diag(var(mu)/var(y-mu)), 2), fill=TRUE)
cat("Correlation between two tasks:", cor(y[, 1], y[, 2]), fill=TRUE)

xtest = x[-(1:ntrain), ]
ytest = y[-(1:ntrain), ]

x = x[1:ntrain, ]
y = y[1:ntrain, ]

# Now, we can fit a ptLasso multiresponse model:
fit = ptLassoMult(x, y, alpha = 0.5, type.measure = "mse")
# plot(fit) # to see all of the cv.glmnet models trained
predict(fit, xtest) # to predict on new data
predict(fit, xtest, ytest=ytest) # if ytest is included, we also measure performance

}
\references{
Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization paths for generalized linear models via coordinate descent. Journal of Statistical Software, 33(1), 1-22.
}
\seealso{
\code{\link{glmnet}}
}
