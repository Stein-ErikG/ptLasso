% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/cv.ptLasso.R
\name{cv.ptLasso}
\alias{cv.ptLasso}
\title{Cross-validation for ptLasso}
\usage{
cv.ptLasso(
  x,
  y,
  groups = NULL,
  alphalist = seq(0, 1, length = 11),
  family = c("gaussian", "multinomial", "binomial", "cox"),
  use.case = c("inputGroups", "targetGroups"),
  type.measure = c("default", "mse", "mae", "auc", "deviance", "class", "C"),
  nfolds = 10,
  foldid = NULL,
  verbose = FALSE,
  fitall = NULL,
  fitind = NULL,
  s = "lambda.min",
  which = "parms.min",
  alphahat.choice = "overall",
  fit.method = "glmnet",
  ...
)
}
\arguments{
\item{x}{\code{x} matrix as in \code{ptLasso}.}

\item{y}{\code{y} matrix as in \code{ptLasso}.}

\item{groups}{A vector of length nobs indicating to which group each observation belongs. For data with k groups, groups should be coded as integers 1 through k.}

\item{alphalist}{A vector of values of the pretraining hyperparameter alpha. Defaults to \code{seq(0, 1, length.out=11)}.}

\item{family}{Response type as in \code{ptLasso}.}

\item{use.case}{The type of grouping observed in the data. Can be one of "inputGroups" or "targetGroups".}

\item{type.measure}{Measure computed in \code{cv.glmnet}, as in \code{ptLasso}.}

\item{nfolds}{Number of folds for CV (default is 10). Although \code{nfolds}can be as large as the sample size (leave-one-out CV), it is not recommended for large datasets. Smallest value allowable is \code{nfolds = 3}.}

\item{foldid}{An optional vector of values between 1 and \code{nfold} identifying what fold each observation is in. If supplied, \code{nfold} can be missing.}

\item{verbose}{If \code{verbose=1}, print a statement showing which model is currently being fit.}

\item{fitall}{An optional cv.glmnet (or cv.sparsenet) object specifying the overall model.}

\item{fitind}{An optional list of cv.glmnet (or cv.sparsenet) objects specifying the individual models.}

\item{s}{For \code{fit.method = "glmnet"} only. The choice of lambda to be used by all models when estimating the CV performance for each choice of alpha. Defaults to "lambda.min". May be "lambda.1se", or a numeric value. (Use caution when supplying a numeric value: the same lambda will be used for all models.)}

\item{which}{For \code{fit.method = "sparsenet"} only. The choice of lambda and gamma to be used by all models when estimating the CV performance for each choice of alpha. Defaults to "parms.min". May also be "parms.1se".}

\item{alphahat.choice}{When choosing alphahat, we may prefer the best performance using all data (\code{alphahat.choice = "overall"}) or the best average performance across groups (\code{alphahat.choice = "mean"}). This is particularly useful when \code{type.measure} is "auc" or "C". These measures look at pairwise comparisons, and therefore are likely to be quite different when using the entire dataset (all pairwise comparisons) and individual groups (comparisons within groups only).}

\item{\dots}{Additional arguments to be passed to the cv.glmnet function. Some notable choices are \code{"trace.it"} and \code{"parallel"}. If \code{trace.it = TRUE}, then a progress bar is displayed for each call to \code{cv.glmnet}; useful for big models that take a long time to fit. If \code{parallel = TRUE}, use parallel \code{foreach} to fit each fold.  Must register parallel before hand, such as \code{doMC} or others. Importantly, \code{"ptLasso"} does not support the arguments \code{"intercept"}, \code{"offset"}, \code{"fit"} and \code{"check.args"}.}
}
\value{
An object of class \code{"cv.ptLasso"}, which is a list with the ingredients of the cross-validation fit.
\item{call}{The call that produced this object.}
\item{alphahat}{Value of \code{alpha} that optimizes CV performance on all data.}
\item{varying.alphahat}{Vector of values of \code{alpha}, the kth of which optimizes performance for group k.}
\item{alphalist}{Vector of all alphas that were compared.}
\item{errall}{CV performance for the overall model.}
\item{errpre}{CV performance for the pretrained models (one for each \code{alpha} tried).}
\item{errind}{CV performance for the individual model.}
\item{fit}{List of \code{ptLasso} objects, one for each \code{alpha} tried.}
}
\description{
Cross-validation for \code{ptLasso}.
}
\details{
This function runs \code{ptLasso} once for each requested choice of alpha, and returns the cross validated performance.
}
\examples{
#### Gaussian example
# Train data
out = gaussian.example.data()
x = out$x; y=out$y; groups = out$group;

# Test data
outtest = gaussian.example.data()
xtest=outtest$x; ytest=outtest$y; groupstest=outtest$groups

# Model fitting
cvfit = cv.ptLasso(x, y, groups = groups, family = "gaussian", type.measure = "mse")
cvfit
# plot(cvfit) # to see CV performance as a function of alpha 
predict(cvfit, xtest, groupstest, ytest=ytest, s="lambda.min")

# Repeat, but this time use sparsenet instead of glmnet:
cvfit = cv.ptLasso(x, y, groups = groups, family = "gaussian", type.measure = "mse",
                   fit.method = "sparsenet", which = "parms.min")
cvfit
# plot(cvfit) # to see CV performance as a function of alpha 
predict(cvfit, xtest, groupstest, ytest=ytest, s="lambda.min")

#### Binomial example
# Train data
out = binomial.example.data()
x = out$x; y=out$y; groups = out$group

# Test data
outtest = binomial.example.data()
xtest=outtest$x; ytest=outtest$y; groupstest=outtest$groups

# Model fitting
cvfit = cv.ptLasso(x, y, groups = groups, family = "binomial",
                   type.measure = "auc", nfolds=3, verbose=TRUE, alphahat.choice="mean")
cvfit
# plot(cvfit) # to see CV performance as a function of alpha 
predict(cvfit, xtest, groupstest, ytest=ytest, s="lambda.1se")

\dontrun{
### Model fitting with parallel = TRUE
require(doMC)
registerDoMC(cores = 4)
outtest = binomial.example.data()
xtest=outtest$x; ytest=outtest$y; groupstest=outtest$groups
cvfit = cv.ptLasso(x, y, groups = groups, family = "binomial", type.measure = "mse", parallel=TRUE)
}

}
\seealso{
\code{\link{ptLasso}} and \code{\link{plot.cv.ptLasso}}.
}
