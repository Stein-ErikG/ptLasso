% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ptLasso.R
\name{ptLasso}
\alias{ptLasso}
\title{Fit a pretrained lasso model using glmnet.}
\usage{
ptLasso(
  x,
  y,
  groups,
  alpha = 0.5,
  family = c("gaussian", "multinomial", "binomial", "cox"),
  type.measure = c("default", "mse", "mae", "auc", "deviance", "class", "C"),
  use.case = c("inputGroups", "targetGroups"),
  overall.lambda = "lambda.1se",
  overall.parms = c("parms.1se", "parms.min"),
  fit.method = c("glmnet", "sparsenet"),
  foldid = NULL,
  nfolds = 10,
  standardize = TRUE,
  verbose = FALSE,
  weights = NULL,
  penalty.factor = rep(1, nvars),
  fitall = NULL,
  fitind = NULL,
  ...
)
}
\arguments{
\item{x}{input matrix, of dimension nobs x nvars; each row is an observation vector. Can be in sparse matrix format (inherit from class '"sparseMatrix"' as in package 'Matrix'). Requirement: 'nvars >1'; in other words, 'x' should have 2 or more columns.}

\item{y}{response variable. Quantitative for 'family="gaussian"'. For 'family="binomial"' should be either a factor with two levels, or a two-column matrix of counts or proportions (the second column is treated as the target class; for a factor, the last level in alphabetical order is the target class). For 'family="multinomial"', can be a 'nc>=2' level factor, or a matrix with 'nc' columns of counts or proportions. For either '"binomial"' or '"multinomial"', if 'y' is presented as a vector, it will be coerced into a factor. For 'family="cox"', preferably a 'Surv' object from the survival package: see Detail section for more information. For 'family="mgaussian"', 'y' is a matrix of quantitative responses.}

\item{groups}{A vector of length nobs indicating to which group each observation belongs. For data with k groups, groups should be coded as integers 1 through k.}

\item{alpha}{The pretrained lasso hyperparameter, with \eqn{0\le\alpha\le 1}.}

\item{family}{Either a character string representing one of the built-in families, or else a 'glm()' family object. For more information, see Details section below or the documentation for response type (above).}

\item{type.measure}{loss to use for cross-validation within each individual, overall, or pretrained lasso model. Currently five options, not all available for all models. The default is 'type.measure="deviance"', which uses squared-error for gaussian models (a.k.a 'type.measure="mse"' there), deviance for logistic and poisson regression, and partial-likelihood for the Cox model. 'type.measure="class"' applies to binomial and multinomial logistic regression only, and gives misclassification error. 'type.measure="auc"' is for two-class logistic regression only, and gives area under the ROC curve. 'type.measure="mse"' or 'type.measure="mae"' (mean absolute error) can be used by all models except the '"cox"'; they measure the deviation from the fitted mean to the response. 'type.measure="C"' is Harrel's concordance measure, only available for 'cox' models.}

\item{use.case}{The type of grouping observed in the data. Can be one of "inputGroups" or "targetGroups".}

\item{overall.lambda}{For 'fit.method = "glmnet"' only. The choice of lambda to be used by the overall model when defining the offset and penalty factor for pretrained lasso. Defaults to "lambda.1se", but "lambda.min" is another good option. If known in advance, can alternatively supply a numeric value.}

\item{overall.parms}{For 'fit.method = "sparsenet"' only. The choice of lambda and gamma to be used by the overall model when defining the offset and penalty factor for pretrained lasso. Can be "parms.1se" or "parms.min". Defaults to "parms.1se".}

\item{fit.method}{"glmnet" or "sparsenet". Defaults to "glmnet". If 'fit.method = "glmnet"', then \code{"cv.glmnet"} will be used to train models. If 'fit.method = "sparsenet"', \code{"cv.sparsenet"} will be used. The use of sparsenet is available only when 'family = "gaussian"'.}

\item{foldid}{An optional vector of values between 1 and \code{nfold} identifying what fold each observation is in. If supplied, \code{nfold} can be missing.}

\item{nfolds}{Number of folds for CV (default is 10). Although \code{nfolds}can be as large as the sample size (leave-one-out CV), it is not recommended for large datasets. Smallest value allowable is \code{nfolds = 3}.}

\item{standardize}{Should the predictors be standardized before fitting (default is TRUE). If \code{fit.method = "sparsenet"}, standardize must be TRUE.}

\item{verbose}{If \code{verbose=1}, print a statement showing which model is currently being fit with \code{cv.glmnet}.}

\item{weights}{observation weights. Default is 1 for each observation.}

\item{penalty.factor}{Separate penalty factors can be applied to each coefficient. This is a number that multiplies 'lambda' to allow differential shrinkage. Can be 0 for some variables,  which implies no shrinkage, and that variable is always included in the model. Default is 1 for all variables (and implicitly infinity for variables listed in 'exclude'). For more information, see \code{?glmnet}. For pretraining, the user-supplied penalty.factor will be multiplied by the penalty.factor computed by the overall model.}

\item{fitall}{An optional cv.glmnet (or cv.sparsenet) object specifying the overall model.}

\item{fitind}{An optional list of cv.glmnet (or cv.sparsenet) objects specifying the individual models.}

\item{\dots}{Additional arguments to be passed to the cv.glmnet (or cv.sparsenet) functions. Notable choices include \code{"trace.it"} and \code{"parallel"}. If \code{trace.it = TRUE}, then a progress bar is displayed for each call to \code{"cv.glmnet"}; useful for big models that take a long time to fit. If \code{parallel = TRUE}, use parallel \code{foreach} to fit each fold.  Must register parallel before hand, such as \code{doMC} or others. ptLasso does not support the arguments \code{intercept}, \code{offset}, \code{fit} and \code{check.args}.}
}
\value{
An object of class \code{"ptLasso"}, which is a list with the ingredients of the fitted models.
\item{call}{The call that produced this object.}
\item{k}{The number of groups.}
\item{alpha}{The value of alpha used for pretraining.}
\item{group.levels}{IDs for all of the groups used in training.}
\item{fitall}{A fitted \code{cv.glmnet} or \code{cv.sparsenet} object trained using the full data.}
\item{fitpre}{A list of fitted (pretrained) \code{cv.glmnet} or \code{cv.sparsenet} objects, one trained with each data group.}
\item{fitind}{A list of fitted \code{cv.glmnet} or \code{cv.sparsenet} objects, one trained with each group.}
\item{fitall.lambda}{For 'fit.method = "glmnet"'. Lambda used with fitall, to compute the offset for pretraining.}
\item{fitall.which}{For 'fit.method = "sparsenet"' only. Gamma and lambda choices used with fitall, to compute the offset for pretraining.}
\item{y.mean}{Gaussian outcome only; mean of y for the training data, used for prediction.}
}
\description{
Fits a pretrained lasso model using the glmnet package, for a fixed choice of the pretraining hyperparameter alpha. Additionally fits an "overall" model (using all data) and "individual" models (use each individual group). Can fit input-grouped data with Gaussian, multinomial, binomial or Cox outcomes, and target-grouped data, which necessarily has a multinomial outcome. Many ptLasso arguments are passed directly to glmnet (or sparsenet), and therefore the glmnet (sparsenet) documentation is another good reference for ptLasso.
}
\examples{
# Gaussian example

out = gaussian.example.data()
x = out$x; y=out$y; groups = out$group

# Test data
outtest = gaussian.example.data()
xtest=outtest$x; ytest=outtest$y; groupstest=outtest$groups

fit = ptLasso(x, y, groups = groups, alpha = 0.5, family = "gaussian", type.measure = "mse")
# plot(fit) to see all of the cv.glmnet models trained
predict(fit, xtest, groupstest, ytest=ytest)

# Binomial example

out = binomial.example.data()
x = out$x; y=out$y; groups = out$group

# Test data
outtest = binomial.example.data()
xtest=outtest$x; ytest=outtest$y; groupstest=outtest$groups

fit = ptLasso(x, y, groups = groups, alpha = 0.5, family = "gaussian", type.measure = "mse")
# plot(fit) to see all of the cv.glmnet models trained
predict(fit, xtest, groupstest, ytest=ytest)

}
\references{
Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization paths for generalized linear models via coordinate descent. Journal of Statistical Software, 33(1), 1-22.
Mazumder, Rahul, Jerome H. Friedman, and Trevor Hastie. "Sparsenet: Coordinate descent with nonconvex penalties." Journal of the American Statistical Association 106.495 (2011): 1125-1138.
}
\seealso{
\code{\link{glmnet}}, \code{\link{sparsenet}}
}
