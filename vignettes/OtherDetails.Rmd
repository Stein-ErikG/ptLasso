---
title: "Other Details"
output: 
  bookdown::pdf_document2:
   toc: false
bibliography: ptLasso.bib
vignette: >
  %\VignetteIndexEntry{Other details}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  % \VignetteDepends{rpart}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r, echo=FALSE}
require(ptLasso)

set.seed(1234)

out = gaussian.example.data()
x = out$x; y = out$y; groups = out$groups

outtest = gaussian.example.data()
xtest = outtest$x; ytest = outtest$y; groupstest = outtest$groups
```

## Choosing $\alpha$, the pretraining hyperparameter
Selecting the hyperparameter $\alpha$ is an important part of pretraining. The simplest way to do this is to use `cv.ptLasso` -- this will automatically perform pretraining for a range of $\alpha$ values and return the CV performance for each. The default values for $\alpha$  are $0, 0.1, 0.2, \dots, 1$. 

```{r}
cvfit <- cv.ptLasso(x, y, groups)
cvfit
```

Of course, you can specify the values of $\alpha$ to consider:
```{r}
cvfit <- cv.ptLasso(x, y, groups, alphalist = c(0, 0.5, 1))
cvfit
```

At prediction time, `cv.ptLasso` uses the $\alpha$ that had the best CV performance on average across all groups. We could instead choose to use a different $\alpha$ for each group, as `cv.ptLasso` already figured out which $\alpha$ optimizes the CV performance for each group. To use group-specific values of $\alpha$, specify `alphatype = "varying"` at prediction time. In this example, the best group-specific $\alpha$ values all happen to be $0.5$ -- the same as the overall $\alpha$.
```{r}
###############################################
# Common alpha for all groups:
###############################################
predict(cvfit, xtest, groupstest, ytest=ytest)

###############################################
# Different alpha for each group:
###############################################
predict(cvfit, xtest, groupstest, ytest=ytest, alphatype = "varying")
```

## Choosing $\lambda$, the lasso hyperparameter, for the first stage of pretraining
The first step of pretraining fits the overall model with `cv.glmnet` and selects a model along the $\lambda$ path. The second stage uses the overall model's support and predictions to train the group-specific models.

So, at train time, we need to know choose a value of $\lambda$ to use for the first stage. This can be specified in `ptLasso` with the argument `overall.lambda`. The default value of `overall.lambda` is "lambda.1se", as we found through simulations and real data examples that this usually had slightly better performance than the natural alternative, "lambda.min". But this is free for you to change: `overall.lambda` can accept "lambda.1se" or "lambda.min" (as in `predict.cv.glmnet`). 

Whatever choice is made at train time will be automatically used at test time, and this cannot be changed. (The fitted model from the second stage of pretraining expects the offset to have been computed using a particular model -- it does not make sense to compute the offset using a different model with a different $\lambda$!)
```{r}
# Default:
fit <- ptLasso(x, y, groups, alpha = 0.5, overall.lambda = "lambda.1se")

# Alternative:
fit <- ptLasso(x, y, groups, alpha = 0.5, overall.lambda = "lambda.min")
```

## Fitting elasticnet or ridge models
By default, `ptLasso` fits lasso penalized models; in `glmnet`, this corresponds to the elasticnet parameter $\alpha_\text{en} = 1$ (where the subscript `en` stands for "elasticnet"). Fitting pretrained elasticnet or ridge models is also possible with `ptLasso`: simply pass in the argument `en.alpha` between $0$ (ridge) and $1$ (lasso). Here is an example using the pretraining hyperparameter $\alpha = 0.5$ and the elasticnet hyperparameter `en.alpha = 0.2`.
```{r}
fit <- ptLasso(x, y, groups, 
               alpha = 0.5,    # pretraining hyperparameter
               en.alpha = 0.2) # elasticnet hyperparameter
```

## Printing progress during model training 
When models take a long time to train, it can be useful to print out progress during training. `ptLasso` has two ways to do this (and they can be combined). First, we can simply print out which model is being fitted using `verbose = TRUE`:
```{r}
fit <- ptLasso(x, y, groups, alpha = 0.5, verbose = TRUE)
```

We can also print out a progress bar for _each model_ that is being fit -- this functionality comes directly from `cv.glmnet`, and follows its notation. (To avoid cluttering this document, we do not run the following example.)
```{r, eval = FALSE}
fit <- ptLasso(x, y, groups, alpha = 0.5, trace.it = TRUE)
```

And of course, we can combine these to print out (1) which model is being trained and (2) the corresponding progress bar.
```{r, eval = FALSE}
fit <- ptLasso(x, y, groups, alpha = 0.5, verbose = TRUE, trace.it = TRUE)
```

## Using individual and overall models that have already been trained
`ptLasso` will fit the overall and individual models. However, if you have already trained the overall or individual models, you can pass them directly to `ptLasso` and avoid refitting them. **`ptLasso` expects that these models were fitted using the same training data that you pass to `ptLasso`, and that they were fitted with the argument `keep = TRUE`.** Here is an example. We will fit an overall model and individual models, and then we will show how to pass them to `ptLasso`. Using `verbose = TRUE` in the call to `ptLasso` shows us what models are being trained (and confirms that we are not refitting the overall and individual models). 
```{r}
overall.model = cv.glmnet(x, y, keep = TRUE)
individual.models = lapply(1:5, 
                           function(kk) cv.glmnet(x[groups == kk, ], 
                                                  y[groups == kk], 
                                                  keep = TRUE))

fit <- ptLasso(x, y, groups, 
               fitoverall = overall.model,
               fitind = individual.models,
               verbose = TRUE)
```


Of course we could pass just the overall *or* individual models to `ptLasso:
```{r}
fit <- ptLasso(x, y, groups, fitoverall = overall.model, verbose = TRUE)
```

```{r}
fit <- ptLasso(x, y, groups, fitind = individual.models, verbose = TRUE)
```

## Fitting the overall model without group-specific intercepts
When we fit the overall model with input grouped data, we solve the following:
\begin{equation}
 	\hat{\mu_0}, \hat{\theta_2}, \dots, \hat{\theta_K}, \hat{\beta_0} = \arg \min_{\mu, \theta_2, \dots, \theta_k, \beta} \frac{1}{2} \sum_{k=1}^K \| y_k - \left(\mu \mathbf{1} + \theta_k \mathbf{1} + X_k \beta\right) \|_2^2 + \lambda ||\beta||_1,
\end{equation}
where $\hat{\theta_1}$ is defined to be $0$. If we do not want a separate intercept for each group ($\theta_1, \dots, \theta_K$), we can instead fit the following:
\begin{equation}
 	\hat{\mu_0}, \hat{\beta_0} = \arg \min_{\mu, \beta} \frac{1}{2} \sum_{k=1}^K \| y_k - \left(\mu \mathbf{1} + X_k \beta\right) \|_2^2 + \lambda ||\beta||_1.
\end{equation}
This may be useful in settings where the groups are different between train and test sets and we show an example in the section "Different groups in train and test data". To do this, use the argument `group.intercepts = FALSE`. In our toy example, omitting the group-specific intercepts results in slightly worse CV performance; we expect this to be the case more generally.

```{r}
cvfit <- cv.ptLasso(x, y, groups, group.intercepts = FALSE)
cvfit
```

## Arguments for use in `cv.glmnet`
Because model fitting is done with `cv.glmnet`, `ptLasso` can take and pass arguments to `glmnet`. Notable choices include `penalty.factor`, `weights`, `upper.limits`, `lower.limits` and `en.alpha` (known as `alpha` in `glmnet`). Please refer to the `glmnet` documentation for more information on their use.

`ptLasso` does not support the arguments `intercept`, `offset`, `fit` and `check.args`.

## Parallelizing model fitting
For large datasets, we can parallelize model fitting within the calls to `cv.glmnet`. As in `cv.glmnet`, pass the argument `parallel = TRUE`, and register parallel beforehand:

```{r, eval=FALSE}
require(doMC)
registerDoMC(cores = 4)
fit = ptLasso(x, y, groups = groups, family = "gaussian", type.measure = "mse", 
              parallel=TRUE)
```
